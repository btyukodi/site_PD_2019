import parameters as pr
import shelve
import gdbm
from helpers import get_edges_between_hubs
import os
import subprocess
import numpy as np
import energy
import platform
import Utils as ut
import jobs
import copy
import parameters as pr
import config
import time

def get_data_location(run_id):
    collection =pr.get_db_collection()
    run_entry = collection.find_one({"_id":run_id})

    #host = platform.node()

    #data_root_conf = pr.get_data_root_config()
    #data_root = data_root_conf.find_one({"host_from":host, "host_to": run_entry['host']})['data_root'] 

    data_path = config.data_root + run_entry['data_location']   
    #return (run_entry['host'], data_path) #<---- host not needed any more here
    return (run_entry['host'], data_path)    

def get_simulation_shelf(run_id=None, data_path=None):
    """Returns a shelf pointing to the data given by either run_id or the path where it's stored"""
    if (run_id and data_path):
        raise ValueError("Give either run_id or data_path")
    if run_id:
        #collection =pr.get_db_collection()
        #run_entry = collection.find_one({"_id":run_id})
        #data_path = run_entry['data_root'] + run_entry['data_location']   
        data_path = get_data_location(run_id)[1] 
    #print data_path
    db = gdbm.open(data_path+'/Capsid','r')
    return shelve.Shelf(db)

def get_group(group_id):
    """Returns a list of run_ids having group_id"""
    collection = pr.get_db_collection()
    cursor = collection.find({"group_id":group_id})
    runs = []
    for c in cursor:
        runs.append(c["_id"])
    return runs

def get_single_capsid_snapshot(run_id=None, data_path=None, timestep=-1):
    """Returns a capsid object from the datafile at specified timestep"""
    if (run_id and data_path):
        raise ValueError("Give either run_id or data_path")
    if run_id:
        shelf = get_simulation_shelf(run_id=run_id)
    if data_path:
        shelf = get_simulation_shelf(data_path=data_path)
    if timestep==-1:
        timestep = sorted([int(k) for k in shelf.keys()])[-1]        
    capsid = shelf[str(timestep)]
    shelf.close()
    return capsid

def convert_to_lammps(run_id=None, data_path=None, freq=1):
    shelf = get_simulation_shelf(run_id, data_path)
    keys = np.sort(np.array(shelf.keys()).astype(int))[::freq].astype(str)
    n_subunits = []
    n_contacts = []
    E_el = []
    print len(keys)
    if run_id:
        host, data_path = get_data_location(run_id)

    for key in keys:
        if int(key)%1000 ==0:
            print key
        capsid = shelf[key]
        lammps_str, bond_energy, stretch_energy, bend_energy, NS, NC = convert_single_capsid_to_lammps(capsid)
        n_subunits.append([int(key), NS])
        n_contacts.append([int(key), NC])
        #print bond_energy[1]
        E_el.append([int(key), sum(bond_energy[1])])

        f = open(data_path+'/lammps_'+key+'.dat', 'w')
        f.write(lammps_str)
        f.close()
        np.savetxt(data_path+'/bond_energy_'+key+'.dat', bond_energy.T)
        np.savetxt(data_path+'/stretch_energy_'+key+'.dat', stretch_energy.T)
        np.savetxt(data_path+'/bend_energy_'+key+'.dat', bend_energy.T)

        if int(key)%100==0:
            np.savetxt(data_path+'/n_subunits.dat', np.array(n_subunits).T)       
            np.savetxt(data_path+'/n_contacts.dat', np.array(n_contacts).T)    
            np.savetxt(data_path+'/elastic_energy.dat', np.array(E_el).T)  


    np.savetxt(data_path+'/n_subunits.dat', np.array(n_subunits).T)       
    np.savetxt(data_path+'/n_contacts.dat', np.array(n_contacts).T)    
    np.savetxt(data_path+'/elastic_energy.dat', np.array(E_el).T)                            
    shelf.close()   


def convert_single_capsid_to_lammps(capsid):
    hubs = capsid.hubs
    edges = capsid.get_edges()
    xlim = '%.3f' %12.000
    
    lammps_str = ('LAMMPSDescription-Generated by btyukodi\n\n'+
                 str(len(hubs))+' atoms\n'+
                 'BBB bonds\n\n'+
                 '1 atom types\n'+
                 '1 bond types\n\n'+
                 '  -'+xlim+'    '+xlim+' xlo xhi\n'+
                 '  -'+xlim+'    '+xlim+' ylo yhi\n'+
                 '  -'+xlim+'    '+xlim+' zlo zhi\n\n'+
                 'Atoms\n\n')
    for i in range(len(hubs)):
        vertex = hubs[i].vertices[0]
        x, y, z = vertex.x, vertex.y, vertex.z
        lammps_str+=str(i+1)+' 1    '+'%.3f'%x+'    '+'%.3f'%y+'    '+'%.3f'%z+'\n'

    lammps_str+='\nBonds\n\n'   
    bond_energy = []
    stretch_energy = []
    bend_energy = []
    n_subunits = len(capsid.subunits)
    n_contacts = sum([(not e.is_surface_edge()) for e in capsid.get_edges()])/2
    ind=1
    for i in range(len(hubs)):
        for j in range(i+1, len(hubs)):
            edge = get_edges_between_hubs(hubs[i], hubs[j])
            if len(edge)>0:
                #print i, j, edge
                lammps_str+=str(ind)+' 1 '+str(i+1)+' '+str(j+1)+'\n'
                #Eb = sum([energy.edge_energy(e) for e in edge]) 


                Es = sum([energy.edge_stretch_energy(e) for e in edge])
                Ebend = energy.bending_energy(edge[0])
                Eb = Es+Ebend
                bond_energy.append([ind, Eb])
                stretch_energy.append([ind, Es])
                bend_energy.append([ind, Ebend])

                ind+=1
    return lammps_str[:-1].replace('BBB',str(ind-1)), np.array(bond_energy).T, np.array(stretch_energy).T, np.array(bend_energy).T, n_subunits, n_contacts


def ovito(run_id=None, data_path=None):
    """Open a run in Ovito. Run it locally and mount remote drives"""
    if (run_id and data_path):
        raise ValueError("Give either run_id or data_path")  
    if data_path:
        host = platform.node()
        path=data_path #still not OK
    if run_id:
        host, path = get_data_location(run_id)
    ##if host=='btyukodi-MS-7B09':
        #this should aready be in data root
    ##    path = '/home/btyukodi/assembly_MC/'+path
    ##if host=='hpcc.brandeis.edu': 
    ##    path = '/media/remote_mnt/'+path
    fname = filter(lambda x: 'lammps' in x, os.listdir(path))[0]
    cmd = 'ovito '+path+'/'+fname
    print cmd
    os.system(cmd)

def plot_capsid(capsid):
    tmp_folder = ('%.10f' % time.time()).replace('.','')+'/'
    lammps_str = convert_single_capsid_to_lammps(capsid)[0]
    ut.ensure_dir('tmp/'+tmp_folder)
    f = open('tmp/'+tmp_folder+'lammps_0.dat', 'w')
    f.write(lammps_str)
    f.close()
    ovito(data_path='tmp/'+tmp_folder)
    os.system("rm -r "+'tmp/'+tmp_folder)

def pack_data(run_ids, tar_filename, include_capsid=True):
    datafiles_abs = []
    for rid in run_ids:
        loc = get_data_location(rid)[1] 
        datafiles = subprocess.check_output("ls "+loc, shell=True).split('\n')[:-1]
        if not include_capsid:
            datafiles.remove('Capsid')
        datafiles_abs = datafiles_abs + [loc+'/'+d for d in datafiles]
		
    f = open(tar_filename+'.tmp', 'w')
    f.write('\n'.join(datafiles_abs))
    f.close()
    os.system("cat "+tar_filename+".tmp | tar cfz "+tar_filename+" -T -")  
    os.system("rm "+tar_filename+".tmp")


def clone_data(run_id, destination_root='/scratch/btyukodi/data/'):
#    """clone database entry and copy data. Return cloned run_id"""
    collection = pr.get_db_collection()
    entry = collection.find_one({"_id":run_id})
    entry.pop("_id", None)
    _id = collection.insert_one(entry).inserted_id
    data_location = '/'.join(list(map(''.join, zip(*[iter(str(_id))]*4))))
    collection.update_one({"_id":_id},{"$set":{'data_location':data_location, 'data_root':destination_root, 'host':'cloned', 'cloned_from':run_id}})
    old_loc = get_data_location(run_id)[1]
    new_loc = get_data_location(_id)[1]
    ut.ensure_dir(new_loc)
    os.system("cp -r "+old_loc+"/. "+new_loc+"/")
    return _id


def move_data(run_id, destination_root, new_host):
#    """clone database entry and copy data. Return cloned run_id"""
#ex.  dp.move_data(ObjectId('5c141fef946df64bdf8690f2'), '/mnt/4TB_HDD/btyukodi/data/', 'btyukodi-MS-7B09')
#find entries before date: https://stackoverflow.com/questions/22026543/find-all-objects-created-before-specified-date
#gen_time = datetime.datetime(2014, 2, 10)
#dummy_id = ObjectId.from_datetime(gen_time)
#result   = collection.find({"_id": {"$lt": dummy_id}})

    old_loc = get_data_location(run_id)[1]
    collection = pr.get_db_collection()
    entry = collection.find_one({"_id":run_id})
    collection.update_one({"_id":run_id},{"$set":{'data_root':destination_root, 'host':new_host}})

    new_loc = get_data_location(run_id)[1]
    #ut.ensure_dir(new_loc)
    os.system("mv "+old_loc+"/ "+new_loc+"/")
    return run_id    

##def process(run_id=None, data_path=None, frequency=1, process_functions=[]):
##    if (run_id and data_path):
##        raise ValueError("Give either run_id or data_path")   
##    if run_id:
##        shelf = get_simulation_shelf(run_id)
##    if data_path:
##        shelf = get_simulation_shelf(data_path)
##    for timestep in sorted([int(k) for k in shelf.keys()])[::frequency]:
##        capsid = shelf[str(timestep)]
##        for process_function in process_functions:
##            #should return something? how to save data processed here? to files or shelf? or to DB?
##            exec(process_function(capsid))

#just hardcode for now
def process(run_id=None, data_path=None):
    print "started"
    if (run_id and data_path):
        raise ValueError("Give either run_id or data_path")   

    convert_to_lammps(run_id, data_path, freq=1)        
    



#parall tempering runs are saved in /tmp folders. This is not convenient for data processing
#this function splits up those simulations to individual database entries as if they were individual runs
def split_parall_tempering_runs(run_id, new_group_id):
    #get temperature values
    collection =pr.get_db_collection()
    run_entry = collection.find_one({"_id":run_id})
    kT_values = run_entry['parameters']['global_params']['kT_values']
    tmp_path = get_data_location(run_id)[1]+'/tmp/'#run_entry['data_root'] + run_entry['data_location']+'/tmp/'

    #create new database entries and folders
    new_data_locations = []
    for kT in kT_values:
        run_params = copy.deepcopy(run_entry['parameters'])
        run_params['global_params']['kT'] = kT
        collection =pr. get_db_collection()
        _id = collection.insert_one({'parameters': run_params, 'status':'parall_temp'}).inserted_id
        data_location = '/'.join(list(map(''.join, zip(*[iter(str(_id))]*4))))
        collection.update_one({"_id":_id},{"$set":{'data_location':data_location, 'group_id':new_group_id, 'parall_temp_source':run_id, 'comments':run_entry['comments'],
            'host':run_entry['host']}})
        new_data_locations.append(data_location)


    #move data to new folders
    for ix, kT in enumerate(kT_values):

        #host = platform.node()

        #data_root_conf = pr.get_data_root_config()
        #data_root = data_root_conf.find_one({"host_from":host, "host_to": run_entry['host']})['data_root'] 

        data_root = config.data_root
        ut.ensure_dir(data_root + new_data_locations[ix])

        old_loc = data_root + run_entry['data_location']+'/tmp/Capsid_kT'+("%.9f" % kT)
        new_loc = data_root + new_data_locations[ix] +"/Capsid"
        os.system("mv "+old_loc+" "+new_loc)

        old_loc = data_root + run_entry['data_location']+'/tmp/Umbrella_hist_kT'+("%.9f" % kT)
        new_loc = data_root + new_data_locations[ix] +"/Umbrella_hist"
        os.system("mv "+old_loc+" "+new_loc)        